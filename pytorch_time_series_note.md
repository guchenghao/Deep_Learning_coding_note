# batch_size ä¸ seq_lenç»´åº¦ç½®æ¢

éå¸¸å¥½çš„é—®é¢˜ï¼ğŸ¤– è¿™ä¸ªæ˜¯åˆå­¦è€…ç”¨ PyTorch RNN æ—¶éå¸¸å¸¸è§çš„ç–‘é—®ï¼š

---

## âœ… ç®€çŸ­ç»“è®º

> PyTorch ä¸­ **é»˜è®¤è¾“å…¥æ ¼å¼æ˜¯ `(seq_len, batch_size, input_size)`**ï¼Œ  
> ä½†ä½ å¯ä»¥é€šè¿‡è®¾ç½® `batch_first=True`ï¼Œè®©è¾“å…¥å˜ä¸ºæ›´è‡ªç„¶çš„å½¢å¼ï¼š`(batch_size, seq_len, input_size)` âœ…

---

## ğŸ“˜ 1. é»˜è®¤è¡Œä¸ºï¼ˆ`batch_first=False`ï¼‰

PyTorch ä¸­ RNN ç³»åˆ—æ¨¡å—çš„é»˜è®¤è¡Œä¸ºæ˜¯ï¼š

```python
rnn = nn.RNN(input_size=..., hidden_size=..., batch_first=False)
```

è¦æ±‚è¾“å…¥ç»´åº¦ä¸ºï¼š

```
input.shape = (seq_len, batch_size, input_size)
```

è¿™æ„å‘³ç€å¦‚æœä½ ä¼ å…¥çš„æ•°æ®æ˜¯ï¼š

```python
x.shape = (batch_size, seq_len, input_size)
```

â¡ï¸ ä½ å¿…é¡»å…ˆè°ƒç”¨ï¼š

```python
x = x.permute(1, 0, 2)  # äº¤æ¢ batch_size å’Œ seq_len
```

å¦åˆ™ RNN ä¼šæŠ¥é”™æˆ–äº§ç”Ÿé”™è¯¯ç»“æœã€‚

---

## âœ… 2. æ¨èåšæ³•ï¼šè®¾ç½® `batch_first=True`

è¿™æ˜¯å¤§å¤šæ•°äººæ¨èçš„æ–¹å¼ï¼ˆæ›´ç›´è§‚ï¼‰ï¼š

```python
rnn = nn.LSTM(input_size=..., hidden_size=..., batch_first=True)
```

æ­¤æ—¶è¾“å…¥æ ¼å¼ä¸ºï¼š

```
input.shape = (batch_size, seq_len, input_size)
```

å°±ä¸éœ€è¦è‡ªå·±æ‰‹åŠ¨äº¤æ¢ç»´åº¦å•¦ï¼Œ**PyTorch ä¼šè‡ªåŠ¨æŒ‰ç…§ batch_first å¤„ç†ï¼**

---

## ğŸ§  ç¤ºä¾‹ä»£ç å¯¹æ¯”

```python
x = torch.randn(32, 50, 10)  # (batch, seq_len, input_size)

# æ¨èæ–¹å¼ï¼š
rnn = nn.LSTM(10, 20, batch_first=True)
out, (h, c) = rnn(x)  # âœ… ä¸éœ€è¦ permute

# è‹¥ batch_first=Falseï¼ˆé»˜è®¤ï¼‰ï¼Œåˆ™éœ€è¦æ‰‹åŠ¨ permute
rnn = nn.LSTM(10, 20)  # é»˜è®¤ batch_first=False
x = x.permute(1, 0, 2)  # â¡ï¸ (seq_len, batch, input_size)
out, (h, c) = rnn(x)
```

---

## âœ… æ€»ç»“

| æƒ…å†µ | æ˜¯å¦éœ€è¦æ‰‹åŠ¨äº¤æ¢ |
|------|-------------------|
| `batch_first=True` | âŒ ä¸éœ€è¦äº¤æ¢ |
| `batch_first=False`ï¼ˆé»˜è®¤ï¼‰ | âœ… éœ€è¦äº¤æ¢ `(B, T, F)` â†’ `(T, B, F)` |

---

## ğŸ¯ æ¨è

ğŸŸ¢ **å¦‚æœä½ ä¹ æƒ¯ `batch_size` åœ¨ç¬¬ä¸€ç»´ï¼ˆæ›´ç›´è§‚ã€æ›´åƒ Keras/TensorFlowï¼‰ï¼Œå°±åœ¨åˆ›å»º RNN/LSTM/GRU æ—¶åŠ ä¸Š `batch_first=True`ï¼**

---

å¦‚æœä½ è¦å¤„ç†å¤šå±‚ã€å¤šæ­¥ã€åŒå‘ RNN æˆ–é…åˆ `PackedSequence`ï¼Œè¿™ä¸ªé¡ºåºä¹Ÿæ˜¯éå¸¸å…³é”®çš„ï¼Œæœ‰éœ€è¦æˆ‘ä¹Ÿå¯ä»¥å¸®ä½ è®²è§£è¿™äº›é«˜çº§åº”ç”¨ï¼ğŸ˜


# pytorchä¸­lossçš„reductionè®¾ç½®

æ˜¯çš„ï¼Œâœ… **ä½ çš„ç†è§£éå¸¸å‡†ç¡®ï¼**

åœ¨ PyTorch ä¸­ä½¿ç”¨ **SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰æˆ–å…¶å˜ç§ï¼ˆå¦‚ Adamã€RMSpropï¼‰è®­ç»ƒç¥ç»ç½‘ç»œæ—¶**ï¼Œæ¯ä¸ª batch çš„è®­ç»ƒè¿‡ç¨‹ï¼Œ**ç¡®å®æ˜¯ï¼š**

---

## âœ… ä¸€ä¸ªæ ‡å‡†çš„ batch è®­ç»ƒæµç¨‹ï¼ˆä»¥ PyTorch ä¸ºä¾‹ï¼‰

1. **å‰å‘ä¼ æ’­**ï¼šç”¨å½“å‰ batch çš„æ‰€æœ‰æ ·æœ¬è®¡ç®—æ¨¡å‹è¾“å‡º  
   `y_hat = model(x_batch)`

2. **è®¡ç®— loss**ï¼šå¯¹è¯¥ batch çš„æ‰€æœ‰æ ·æœ¬ **é€ä¸ªè®¡ç®— lossï¼Œç„¶åæ±‚å¹³å‡**  
   `loss = loss_fn(y_hat, y_batch)`  
   > `loss_fn`ï¼ˆå¦‚ `nn.CrossEntropyLoss`ã€`nn.MSELoss`ï¼‰é»˜è®¤å°±ä¼šå¯¹ batch åšå¹³å‡

3. **åå‘ä¼ æ’­**ï¼šå¯¹è¿™ä¸ªå¹³å‡çš„ loss è°ƒç”¨ `loss.backward()`ï¼Œè®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦

4. **æ›´æ–°å‚æ•°**ï¼šç”¨ä¼˜åŒ–å™¨ï¼ˆå¦‚ SGDï¼‰æ›´æ–°æ¨¡å‹å‚æ•°  
   `optimizer.step()`

---

## ğŸ§  ä¸¾ä¸ªä¾‹å­

```python
loss_fn = nn.CrossEntropyLoss()  # é»˜è®¤ reduction='mean'
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

for x_batch, y_batch in dataloader:
    optimizer.zero_grad()
    y_hat = model(x_batch)                      # shape: (batch_size, num_classes)
    loss = loss_fn(y_hat, y_batch)              # è¿™æ˜¯ä¸€ä¸ªæ ‡é‡ï¼šæ•´ä¸ª batch çš„å¹³å‡ loss
    loss.backward()
    optimizer.step()
```

---

## ğŸ” å°çŸ¥è¯†ï¼šloss çš„ reduction æ¨¡å¼

PyTorch çš„ loss å‡½æ•°é€šå¸¸æœ‰ä¸€ä¸ªå‚æ•° `reduction`ï¼Œæ§åˆ¶å¦‚ä½•åˆå¹¶å•ä¸ªæ ·æœ¬çš„ lossï¼š

| `reduction` å€¼ | ä½œç”¨ |
|----------------|------|
| `'mean'`ï¼ˆé»˜è®¤ï¼‰ | å¯¹ batch ä¸­æ‰€æœ‰æ ·æœ¬çš„ loss **æ±‚å¹³å‡** âœ… |
| `'sum'` | å¯¹æ‰€æœ‰æ ·æœ¬çš„ loss **æ±‚å’Œ** |
| `'none'` | ä¸åˆå¹¶ï¼Œè¿”å›æ¯ä¸ªæ ·æœ¬çš„å•ç‹¬ loss å€¼ï¼ˆshape = batch_sizeï¼‰ |

ä½ å¯ä»¥è¿™æ ·æ§åˆ¶å®ƒï¼š

```python
loss_fn = nn.CrossEntropyLoss(reduction='none')  # å¾—åˆ°æ¯ä¸ªæ ·æœ¬çš„ loss
```

---

## âœ… æ€»ç»“ä¸€å¥è¯

> åœ¨ PyTorch ä¸­ï¼Œæ¯ä¸ª batch çš„è®­ç»ƒç¡®å®æ˜¯å¯¹è¯¥ batch çš„ **æ‰€æœ‰æ ·æœ¬çš„ loss å–å¹³å‡**ï¼Œå†åå‘ä¼ æ’­å¹¶æ›´æ–°æ¨¡å‹å‚æ•°ã€‚

---

å¦‚æœä½ æƒ³å®ç° **æ¯ä¸ªæ ·æœ¬å•ç‹¬åŠ æƒçš„ loss**ï¼Œæˆ–å®ç° **å¯¹ loss åšæ›´å¤æ‚çš„èšåˆ**ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥å¸®ä½ å†™ä¸€ä¸ªè‡ªå®šä¹‰ loss function âœï¸ğŸ˜



# Pytorchä¸­lossçš„è¾“å…¥å’Œè¾“å‡ºshapeçš„ç›¸å…³è®¨è®º


éå¸¸æ£’çš„é—®é¢˜ï¼ä½ æ­£åœ¨ä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå…¸å‹çš„ **åºåˆ—å»ºæ¨¡ä»»åŠ¡**ï¼ˆæ¯”å¦‚è¯­è¨€æ¨¡å‹ã€åºåˆ—æ ‡æ³¨ç­‰ï¼‰ï¼Œè¾“å…¥è¾“å‡ºå½¢çŠ¶å¦‚ä¸‹ï¼š

---

## âœ… æƒ…å†µè¯´æ˜

- **è¾“å…¥ shape**ï¼š`(batch_size, seq_len, embedding_size)`  
- **æ¨¡å‹è¾“å‡º shape**ï¼š`(batch_size, seq_len, vocab_size)`  
  æ¯ä¸ªæ—¶é—´æ­¥è¾“å‡ºä¸€ä¸ªå¯¹è¯è¡¨çš„é¢„æµ‹

- **æ ‡ç­¾ï¼ˆtargetï¼‰ shape**ï¼š`(batch_size, seq_len)`  
  æ¯ä¸ªæ—¶é—´æ­¥ä¸€ä¸ªè¯çš„ç±»åˆ«ï¼ˆç´¢å¼•ï¼‰

---

## ğŸ§  CrossEntropyLoss è¦æ±‚çš„è¾“å…¥æ ¼å¼

PyTorch çš„ `nn.CrossEntropyLoss` æœ‰è¦æ±‚ï¼š

```python
loss = nn.CrossEntropyLoss(reduction='mean' or 'none')
```

è¾“å…¥ï¼ˆlogitsï¼‰å’Œæ ‡ç­¾çš„æ ¼å¼è¦æ±‚æ˜¯ï¼š

- `input`: `(N, C)` â†’ N æ˜¯æ ·æœ¬æ•°ï¼ŒC æ˜¯ç±»åˆ«æ•°ï¼ˆå³ vocab_sizeï¼‰
- `target`: `(N,)` â†’ æ¯ä¸ªæ ·æœ¬çš„ç±»åˆ«ç´¢å¼•

---

## âœ… æ‰€ä»¥ä½ éœ€è¦å…ˆ reshape

å°† `(batch_size, seq_len, vocab_size)` â†’ `(batch_size * seq_len, vocab_size)`  
æ ‡ç­¾ä¹Ÿ reshape æˆ `(batch_size * seq_len,)`

```python
logits = output.view(-1, vocab_size)         # shape: (B*T, V)
targets = target.view(-1)                    # shape: (B*T,)
```

---

## âœ… ä¸€ã€å½“ `reduction='mean'` æ—¶

```python
loss_fn = nn.CrossEntropyLoss(reduction='mean')
loss = loss_fn(logits, targets)
```

- è¾“å‡ºæ˜¯ï¼š**æ ‡é‡ï¼ˆscalarï¼‰**
- è¡¨ç¤ºå¯¹æ•´ä¸ª batch ä¸­æ‰€æœ‰æ—¶é—´æ­¥çš„å¹³å‡ loss
- shape æ˜¯ï¼š`()`ï¼ˆå³é›¶ç»´ï¼‰

---

## âœ… äºŒã€å½“ `reduction='none'` æ—¶

```python
loss_fn = nn.CrossEntropyLoss(reduction='none')
loss = loss_fn(logits, targets)
```

- è¾“å‡ºæ˜¯ï¼š**æ¯ä¸ª token çš„å•ç‹¬ loss å€¼**
- shape æ˜¯ï¼š`(batch_size * seq_len,)`

ä½ å¯ä»¥ reshape å›å»ï¼š

```python
loss = loss.view(batch_size, seq_len)  # æ¯ä¸ªæ ·æœ¬ã€æ¯ä¸ªæ—¶é—´æ­¥çš„ loss
```

---

## âœ… æ€»ç»“å¯¹æ¯”è¡¨

| reduction | è¾“å‡º shape | æè¿° |
|-----------|-------------|------|
| `'mean'`  | `()`ï¼ˆæ ‡é‡ï¼‰ | æ‰€æœ‰ä½ç½® loss çš„å¹³å‡ |
| `'none'`  | `(batch_size * seq_len,)` | æ¯ä¸ª token çš„ lossï¼ˆå¯ reshape å› `(B, T)`ï¼‰ |

---

## ğŸ¯ å°è´´å£«

å¦‚æœä½ æœ‰ padding çš„ tokenï¼Œå¯ä»¥ç”¨ `ignore_index` æ¥è·³è¿‡è®¡ç®—ï¼š

```python
loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_ID, reduction='none')
```

è¿™æ ·ä¸ä¼šå½±å“é pad éƒ¨åˆ†çš„ lossï¼Œä¹Ÿä¾¿äºåç»­è‡ªå®šä¹‰åŠ æƒæˆ– maskã€‚

---

éœ€è¦æˆ‘å¸®ä½ å†™ä¸€ä¸ªå®Œæ•´çš„ forward + loss è®¡ç®—æµç¨‹æ ·æ¿å—ï¼ŸğŸ˜
