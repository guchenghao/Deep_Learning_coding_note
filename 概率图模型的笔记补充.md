# 概率图模型


## 变量消除法


**变量消除法**（Variable Elimination）是一种在概率图模型中使用的高效推断算法，用于计算边缘概率或条件概率。它通过对图中的随机变量逐步求和（或积分）来消除非关心的变量，从而避免了直接枚举整个状态空间的高昂计算代价。

变量消除法常用于**贝叶斯网络**和**马尔可夫随机场**等场景。

---

### **核心思想**

在概率图模型中，给定联合概率分布 \( P(\mathbf{X}) \)，如果想要计算某些目标变量（如 \( \mathbf{Y} \)）的边缘概率 \( P(\mathbf{Y}) \)，可以通过对其他变量（非目标变量）进行求和来实现：
\[
P(\mathbf{Y}) = \sum_{\mathbf{Z}} P(\mathbf{Y}, \mathbf{Z})
\]
其中：

- \( \mathbf{Y} \) 是目标变量。
- \( \mathbf{Z} \) 是所有非目标变量。

变量消除法通过利用**条件独立性**和图的因子分解结构，分步计算消除每个变量的部分结果，从而减少冗余计算。

---

### **具体步骤**

1. **将联合概率分解为因子表示**：
   根据概率图的结构，将联合概率分布 \( P(\mathbf{X}) \) 分解为若干个因子（如势函数或条件概率表）：
   \[
   P(\mathbf{X}) = \prod_{i} f_i(\mathbf{X}_i)
   \]
   每个因子 \( f_i(\mathbf{X}_i) \) 仅依赖于局部的变量子集 \( \mathbf{X}_i \)。

2. **选择变量消除顺序**：
   确定一个消除变量的顺序（通常通过启发式方法选择）。

3. **依次消除变量**：
   对每个需要消除的变量 \( Z \)，执行以下步骤：
   - 找到所有包含 \( Z \) 的因子。
   - 将这些因子相乘，得到一个新的中间因子。
   - 对 \( Z \) 求和（或积分）以消除 \( Z \)。

4. **重复直到剩下目标变量**：
   依次消除非目标变量后，最终只剩下目标变量及其相关因子。

5. **归一化**（如需要）：
   如果需要计算条件概率，可以通过对结果进行归一化操作。

---

### **举例**

#### 问题

假设我们有以下贝叶斯网络，变量分别为 \( A \), \( B \), \( C \), \( D \)，其条件概率分布如下：

- \( P(A) \)
- \( P(B | A) \)
- \( P(C | A) \)
- \( P(D | B, C) \)

我们想计算 \( P(D) \)，即消除变量 \( A \), \( B \), \( C \)。

---

#### 步骤

1. **联合分布表示**：
   根据贝叶斯网络的结构，联合分布可以分解为：
   \[
   P(A, B, C, D) = P(A) \cdot P(B | A) \cdot P(C | A) \cdot P(D | B, C)
   \]

2. **选择消除顺序**：
   假设消除顺序为 \( A \to B \to C \)。

3. **依次消除变量**：

   - **消除 \( A \)**：
     找到所有包含 \( A \) 的因子：\( P(A), P(B | A), P(C | A) \)。
     计算中间结果：
     \[
     f_1(B, C) = \sum_A P(A) \cdot P(B | A) \cdot P(C | A)
     \]

   - **消除 \( B \)**：
     找到所有包含 \( B \) 的因子：\( f_1(B, C), P(D | B, C) \)。
     计算中间结果：
     \[
     f_2(C, D) = \sum_B f_1(B, C) \cdot P(D | B, C)
     \]

   - **消除 \( C \)**：
     找到所有包含 \( C \) 的因子：\( f_2(C, D) \)。
     计算最终结果：
     \[
     P(D) = \sum_C f_2(C, D)
     \]

4. **结果**：
   最终得到 \( P(D) \) 的值。

---

### **优势**

1. **高效利用独立性**：
   变量消除法利用图的条件独立性和因子分解形式，避免了直接枚举所有变量组合。

2. **递归性强**：
   每一步的计算结果是一个新的因子，递归地减少问题规模。

3. **适用范围广**：
   可用于计算边缘概率、条件概率等。

---

### **劣势**

1. **消除顺序影响效率**：
   不同的变量消除顺序会导致不同的中间因子的大小，从而显著影响计算效率。通常需要用启发式方法（如最小填充边法）来选择消除顺序。

2. **中间因子可能过大**：
   如果图结构复杂，消除某些变量可能会生成非常大的中间因子，导致计算和存储的开销增大。

3. **无法处理动态问题**：
   对于随时间变化的动态系统（如动态贝叶斯网络），变量消除法效率较低。

---

### **实际应用**

1. **贝叶斯网络推断**：
   - 用于计算边缘概率和条件概率。
   - 如医疗诊断问题中的疾病概率计算。

2. **马尔可夫随机场**：
   - 在图像处理和计算机视觉中，用于计算像素标签的边缘分布。

3. **模型简化**：
   - 在复杂概率图模型中，通过消除不相关变量简化计算。

---

### **总结**

变量消除法是一种高效的推断算法，通过逐步消除非目标变量来计算概率分布。它的核心在于利用概率图模型的条件独立性和因子分解形式，减少计算复杂度。尽管消除顺序选择和中间因子大小可能带来挑战，但其思想简单且适用于多种场景，是概率推断中的重要工具。





-------------------------------


## Viterbi 算法


**Viterbi算法**是一种动态规划算法，用于在隐藏马尔可夫模型（Hidden Markov Model, HMM）中找到最可能的隐藏状态序列（即**最优路径**）。它常被用于标注问题，例如自然语言处理中的词性标注、语音识别中的语音转录等。

---

### **隐藏马尔可夫模型 (HMM)** 简要回顾

HMM由以下部分组成：

1. **隐藏状态集合** \( S = \{s_1, s_2, \dots, s_N\} \)：隐藏状态是我们需要推断的目标。
2. **观测集合** \( O = \{o_1, o_2, \dots, o_T\} \)：可以观察到的输出。
3. **初始状态概率** \( \pi = \{\pi_1, \pi_2, \dots, \pi_N\} \)：每个隐藏状态的初始概率。
4. **状态转移概率矩阵** \( A = [a_{ij}] \)：从状态 \( s_i \) 转移到状态 \( s_j \) 的概率。
5. **观测概率矩阵** \( B = [b_{j}(o)] \)：在状态 \( s_j \) 下生成观测 \( o \) 的概率。

目标是：在给定观测序列 \( O = \{o_1, o_2, \dots, o_T\} \) 和 HMM 参数 \( \lambda = (\pi, A, B) \) 时，找到最可能的隐藏状态序列 \( Q = \{q_1, q_2, \dots, q_T\} \)。

---

### **Viterbi算法的目标**

Viterbi算法计算：
\[
Q^*= \arg\max_{Q} P(Q | O, \lambda)
\]
即，找到给定观测序列 \( O \) 的条件下，最可能的隐藏状态序列 \( Q^* \)。

通过贝叶斯公式：
\[
P(Q | O, \lambda) \propto P(O | Q, \lambda) P(Q | \lambda)
\]

由于 HMM 的结构条件独立性，可以将问题分解为最大化：
\[
P(Q | O, \lambda) = \pi_{q_1} b_{q_1}(o_1) \prod_{t=2}^T a_{q_{t-1}, q_t} b_{q_t}(o_t)
\]

---

### **Viterbi算法的步骤**

Viterbi算法利用动态规划方法逐步计算隐藏状态序列的最优概率。其主要步骤如下：

#### **1. 初始化**

定义一个动态规划表 \( \delta_t(j) \)，表示在时间 \( t \) 时刻，终止于状态 \( s_j \) 的最优路径的概率：
\[
\delta_1(j) = \pi_j \cdot b_j(o_1), \quad \forall j \in \{1, 2, \dots, N\}
\]
并存储最优路径的前导状态：
\[
\psi_1(j) = 0, \quad \forall j \in \{1, 2, \dots, N\}
\]

#### **2. 递推**

对于每个时间 \( t = 2, 3, \dots, T \) 和每个状态 \( s_j \)，计算：
\[
\delta_t(j) = \max_{i} \left[ \delta_{t-1}(i) \cdot a_{i,j} \right] \cdot b_j(o_t)
\]
这里：

- \( \delta_{t-1}(i) \cdot a_{i,j} \) 表示从状态 \( s_i \) 转移到状态 \( s_j \) 的概率。
- \( b_j(o_t) \) 表示状态 \( s_j \) 生成观测 \( o_t \) 的概率。

同时，记录每个状态的前导状态：
\[
\psi_t(j) = \arg\max_{i} \left[ \delta_{t-1}(i) \cdot a_{i,j} \right]
\]

#### **3. 终止**

计算整个序列的最大概率和终止状态：
\[
P^*= \max_{j} \delta_T(j)
\]
\[
q_T^* = \arg\max_{j} \delta_T(j)
\]

#### **4. 回溯**

从终止状态 \( q_T^*\) 开始，通过前导指针 \( \psi_t(j) \) 回溯，依次找出最优路径：
\[
q_{t-1}^* = \psi_t(q_t^*), \quad t = T, T-1, \dots, 2
\]

---

### **Viterbi算法的伪代码**

```python
# 输入: 观测序列 O, 状态集合 S, 初始概率 π, 状态转移矩阵 A, 观测概率矩阵 B
# 输出: 最优状态序列 Q*

初始化:
    T = len(O) # 观测序列长度
    N = len(S) # 状态集合大小
    δ = [[0] * N for _ in range(T)] # 动态规划表
    ψ = [[0] * N for _ in range(T)] # 前导指针表

# 1. 初始化
for j in range(N):
    δ[0][j] = π[j] * B[j][O[0]]
    ψ[0][j] = 0

# 2. 递推
for t in range(1, T):
    for j in range(N):
        δ[t][j] = max(δ[t-1][i] * A[i][j] for i in range(N)) * B[j][O[t]]
        ψ[t][j] = argmax(δ[t-1][i] * A[i][j] for i in range(N))

# 3. 终止
P_star = max(δ[T-1][j] for j in range(N))
q_T_star = argmax(δ[T-1][j] for j in range(N))

# 4. 回溯
Q_star = [0] * T
Q_star[T-1] = q_T_star
for t in range(T-2, -1, -1):
    Q_star[t] = ψ[t+1][Q_star[t+1]]

返回 Q_star
```

---

### **实际例子**

#### **假设的HMM模型：**

- **隐藏状态**：天气 \( S = \{\text{晴}, \text{雨}\} \)
- **观测值**：活动 \( O = \{\text{散步}, \text{购物}, \text{清理}\} \)
- **初始概率**：
  \[
  \pi = \{P(\text{晴}) = 0.6, P(\text{雨}) = 0.4\}
  \]
- **状态转移矩阵**：
  \[
  A =
  \begin{bmatrix}
  0.7 & 0.3 \\
  0.4 & 0.6
  \end{bmatrix}
  \]
- **观测概率矩阵**：
  \[
  B =
  \begin{bmatrix}
  0.1 & 0.4 & 0.5 \\
  0.6 & 0.3 & 0.1
  \end{bmatrix}
  \]

#### **观测序列**

\( O = [\text{散步}, \text{购物}, \text{清理}] \)

#### **目标**

使用Viterbi算法找到最可能的天气序列。

---

### **步骤**

1. **初始化**：
   - \( \delta_1(\text{晴}) = 0.6 \times 0.1 = 0.06 \)
   - \( \delta_1(\text{雨}) = 0.4 \times 0.6 = 0.24 \)

2. **递推**：
   - 时间 \( t = 2 \)：
     - \( \delta_2(\text{晴}) = \max[0.06 \times 0.7, 0.24 \times 0.4] \times 0.4 = 0.0384 \)
     - \( \delta_2(\text{雨}) = \max[0.06 \times 0.3, 0.24 \times 0.6] \times 0.3 = 0.0432 \)

   - 时间 \( t = 3 \)：
     - \( \delta_3(\text{晴}) = \max[0.0384 \times 0.7, 0.0432 \times 0.4] \times 0.5 = 0.01344 \)
     - \( \delta_3(\text{雨}) = \max[0.0384 \times 0.3, 0.0432 \times 0.6] \times 0.1 = 0.002592 \)

3. **终止**：
   - \( P^* = \max[0.01344, 0.002592] = 0.01344 \)
   - \( q_T^* = \text{晴} \)

4. **回溯**：
   - 最优路径：\(\{\text{雨}, \text{雨}, \text{晴}\}\)

---

### **总结**

- **输入**：观测序列和HMM参数。
- **输出**：最可能的隐藏状态序列。
- **核心思想**：利用动态规划逐步计算最优路径概率并记录路径信息。
- **优点**：高效，避免了枚举所有可能路径。

Viterbi算法在语音识别、自然语言处理、基因序列分析等领域广泛应用。
