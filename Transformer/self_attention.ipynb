{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4960, 0.5040],\n",
      "         [0.4848, 0.5152]],\n",
      "\n",
      "        [[0.4944, 0.5056],\n",
      "         [0.4923, 0.5077]],\n",
      "\n",
      "        [[0.5221, 0.4779],\n",
      "         [0.5274, 0.4726]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1299, -0.6092, -0.4495, -0.2527],\n",
       "         [-0.1283, -0.6098, -0.4518, -0.2592]],\n",
       "\n",
       "        [[-0.2092, -0.8641, -0.7713, -0.2240],\n",
       "         [-0.2096, -0.8637, -0.7708, -0.2232]],\n",
       "\n",
       "        [[-0.1089, -0.6132, -0.4714, -0.2210],\n",
       "         [-0.1097, -0.6106, -0.4687, -0.2207]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''第一个版本的self-attention'''\n",
    "\n",
    "####################\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    \"\"\"self-attention的第一重境界\"\"\"\n",
    "    def __init__(self, hidden_dim: int =728):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # * Q, K, V的权重矩阵\n",
    "        # * 假设x的shape是(128, 20), nn.Linear的dim是(20, 30), 最终计算结果是(128, 30)\n",
    "        self.query_Q_weight = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_K_weight = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_V_weight = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # * X shape is : (batch_size, seq_len, hidden_dim)\n",
    "        Q_matrix = self.query_Q_weight(x)\n",
    "        K_matrix = self.key_K_weight(x)\n",
    "        V_matrix = self.value_V_weight(x)\n",
    "        \n",
    "        # * Q, K, V shape is : (batch, seq_len, hidden_dim)\n",
    "        # * Attention weights shape is : (batch, seq_len, seq_len)\n",
    "        \n",
    "        attention_matrix = torch.matmul(Q_matrix, K_matrix.transpose(-1, -2))\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_matrix / math.sqrt(self.hidden_dim), dim=-1)\n",
    "        print(attention_weights)\n",
    "        # * output shape is : (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V_matrix)\n",
    "        \n",
    "                \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "X = torch.rand(3, 2, 4)\n",
    "\n",
    "# print(\"X: {}\".format(X))\n",
    "\n",
    "attention_V1 = SelfAttentionV1(4)\n",
    "\n",
    "attention_V1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5215, 0.4785],\n",
      "         [0.5253, 0.4747]],\n",
      "\n",
      "        [[0.5161, 0.4839],\n",
      "         [0.5132, 0.4868]],\n",
      "\n",
      "        [[0.5149, 0.4851],\n",
      "         [0.5078, 0.4922]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4113, -0.3819, -0.2678, -0.0727],\n",
       "         [ 0.4110, -0.3830, -0.2681, -0.0723]],\n",
       "\n",
       "        [[ 0.2601, -0.1874, -0.0258, -0.2135],\n",
       "         [ 0.2594, -0.1877, -0.0255, -0.2140]],\n",
       "\n",
       "        [[ 0.7261, -0.3526, -0.4820, -0.0516],\n",
       "         [ 0.7245, -0.3512, -0.4798, -0.0526]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"对原本的self-attention计算进行效率优化\"\"\"\n",
    "\n",
    "\n",
    "class SelfAttentionV2(nn.Module):\n",
    "    \"\"\"Some Information about SelfAttentionV2\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # * 将Q, K, V3个矩阵合并为一个大矩阵进行计算\n",
    "        self.cal_proj_weight = nn.Linear(hidden_dim, hidden_dim * 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # * X shape is : (batch, seq, dim)\n",
    "        # ! 这种方式只适用于模型比较小的时候\n",
    "        QKV_matrix = self.cal_proj_weight(x)\n",
    "        Q_matrix, K_matrix, V_matrix = torch.split(QKV_matrix, self.hidden_dim, dim=-1)\n",
    "        \n",
    "        \n",
    "        # * Q, K, V shape is : (batch, seq_len, hidden_dim)\n",
    "        # * Attention weights shape is : (batch, seq_len, seq_len)\n",
    "        \n",
    "        attention_matrix = torch.matmul(Q_matrix, K_matrix.transpose(-1, -2))\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_matrix / math.sqrt(self.hidden_dim), dim=-1)\n",
    "        print(attention_weights)\n",
    "        # * output shape is : (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V_matrix)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "X = torch.rand(3, 2, 4)\n",
    "\n",
    "attention_V2 = SelfAttentionV2(4)\n",
    "\n",
    "attention_V2(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "tensor([[[0.3364, 0.3375, 0.3261, 0.0000],\n",
      "         [0.3198, 0.3440, 0.3362, 0.0000],\n",
      "         [0.3247, 0.3414, 0.3339, 0.0000],\n",
      "         [0.3294, 0.3390, 0.3315, 0.0000]],\n",
      "\n",
      "        [[0.5025, 0.4975, 0.0000, 0.0000],\n",
      "         [0.4829, 0.5171, 0.0000, 0.0000],\n",
      "         [0.4916, 0.5084, 0.0000, 0.0000],\n",
      "         [0.4966, 0.5034, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3273, -0.6831],\n",
       "         [-0.4053, -0.5083],\n",
       "         [-0.3276, -0.6836],\n",
       "         [-0.3275, -0.6833]],\n",
       "\n",
       "        [[-0.2727, -0.7993],\n",
       "         [-0.2698, -0.8035],\n",
       "         [-0.2711, -0.8017],\n",
       "         [-0.2718, -0.8006]],\n",
       "\n",
       "        [[-0.3080, -0.6561],\n",
       "         [-0.3080, -0.6561],\n",
       "         [-0.3080, -0.6561],\n",
       "         [-0.3080, -0.6561]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''加入一些关于self-attention的细节'''\n",
    "\n",
    "# * 1. Dropout的位置\n",
    "# * 2. Attention mask, 因为在实际应用过程中，sequence的长度可能是不一样的\n",
    "# * 3. output 矩阵映射（可选）\n",
    "\n",
    "class SelfAttentionV3(nn.Module):\n",
    "    \"\"\"Some Information about SelfAttentionV3\"\"\"\n",
    "    def __init__(self, hiddendim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.hiddendim = hiddendim\n",
    "        self.cal_matrix_weight = nn.Linear(hiddendim, hiddendim * 3)\n",
    "        self.dropout_layer = nn.Dropout(dropout_rate) # * 定义dropout\n",
    "        \n",
    "        # * 可选\n",
    "        self.output_mapping = nn.Linear(hiddendim, hiddendim)\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        QKV_matrix = self.cal_matrix_weight(x)\n",
    "        Q_matrix, K_matrix, V_matrix = torch.split(QKV_matrix, self.hiddendim, dim=-1)\n",
    "        \n",
    "        # * attention_matrix和attention_mask shape is (batch, seq, seq)\n",
    "        attention_matrix = Q_matrix @ K_matrix.transpose(-1, -2) / math.sqrt(self.hiddendim)\n",
    "        \n",
    "        # * mask要加在计算softmax之前\n",
    "        if attention_mask is not None:\n",
    "            attention_matrix = attention_matrix.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "             \n",
    "        # * Attention的Dropout是在Attention层面进行Dropout，因此将Dropout层加在计算完attention_weights之后\n",
    "        attention_weight = torch.softmax(attention_matrix, dim=-1)\n",
    "        print(attention_weight)\n",
    "        attention_weight = self.dropout_layer(attention_weight)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output = attention_weight @ V_matrix\n",
    "        \n",
    "        \n",
    "        output = self.output_mapping(output)\n",
    "            \n",
    "    \n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "X = torch.rand(3, 4, 2)\n",
    "mask = torch.tensor([[1,1,1,0],[1,1,0,0], [1,0,0,0]])\n",
    "\n",
    "print(mask.shape)\n",
    "\n",
    "mask = mask.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "print(mask.shape)\n",
    "attention_V3 = SelfAttentionV3(2)\n",
    "\n",
    "attention_V3(X, mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3425, 0.3439, 0.3136, 0.0000],\n",
      "         [0.3406, 0.3431, 0.3163, 0.0000],\n",
      "         [0.3471, 0.3366, 0.3164, 0.0000],\n",
      "         [0.3427, 0.3436, 0.3137, 0.0000]],\n",
      "\n",
      "        [[0.4733, 0.5267, 0.0000, 0.0000],\n",
      "         [0.4705, 0.5295, 0.0000, 0.0000],\n",
      "         [0.4783, 0.5217, 0.0000, 0.0000],\n",
      "         [0.4637, 0.5363, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5100, -0.8924],\n",
       "         [ 0.5101, -0.8929],\n",
       "         [ 0.5123, -0.8970],\n",
       "         [ 0.5101, -0.8926]],\n",
       "\n",
       "        [[ 0.3029, -0.4966],\n",
       "         [ 0.3026, -0.4956],\n",
       "         [ 0.3036, -0.4985],\n",
       "         [ 0.3017, -0.4929]],\n",
       "\n",
       "        [[ 0.4814, -0.9871],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.4814, -0.9871],\n",
       "         [ 0.4814, -0.9871]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''实际面试时的写法'''\n",
    "\n",
    "class SelfAttentionV4(nn.Module):\n",
    "    \"\"\"Some Information about SelfAttentionV4\"\"\"\n",
    "    def __init__(self, hiddendim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hiddendim = hiddendim\n",
    "        \n",
    "        self.query = nn.Linear(hiddendim, hiddendim)\n",
    "        self.key = nn.Linear(hiddendim, hiddendim)\n",
    "        self.value = nn.Linear(hiddendim, hiddendim)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \n",
    "        Q_matrix = self.query(x)\n",
    "        K_matrix = self.key(x)\n",
    "        V_matrix = self.value(x)\n",
    "        \n",
    "        attention_matrix = Q_matrix @ K_matrix.transpose(-1, -2) / math.sqrt(self.hiddendim)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_matrix = attention_matrix.masked_fill(attention_mask == 0, float(\"-inf\"))\n",
    "        \n",
    "        # * (batch, seq, seq)\n",
    "        attention_weights = torch.softmax(attention_matrix, dim=-1)\n",
    "        print(attention_weights)\n",
    "        attention_weights = self.dropout_layer(attention_weights)\n",
    "        \n",
    "        # * (batch, seq, hiddendim)\n",
    "        output = attention_weights @ V_matrix\n",
    "        \n",
    "        \n",
    "        return output\n",
    "\n",
    "X = torch.rand(3, 4, 2)\n",
    "\n",
    "mask = torch.tensor([[1,1,1,0],[1,1,0,0],[1,0,0,0]])\n",
    "\n",
    "mask = mask.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "\n",
    "attention_V4 = SelfAttentionV4(2)\n",
    "\n",
    "attention_V4(X, mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
