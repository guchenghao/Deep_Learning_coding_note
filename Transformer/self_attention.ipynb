{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5218, 0.4782],\n",
      "         [0.5193, 0.4807]],\n",
      "\n",
      "        [[0.5030, 0.4970],\n",
      "         [0.5044, 0.4956]],\n",
      "\n",
      "        [[0.4886, 0.5114],\n",
      "         [0.4859, 0.5141]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6099, -0.1530, -0.8041,  0.2956],\n",
       "         [-0.6100, -0.1535, -0.8045,  0.2958]],\n",
       "\n",
       "        [[-0.3163, -0.0801, -0.2249,  0.3892],\n",
       "         [-0.3164, -0.0800, -0.2249,  0.3891]],\n",
       "\n",
       "        [[-0.3718, -0.0180, -0.4918,  0.2616],\n",
       "         [-0.3723, -0.0169, -0.4922,  0.2607]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''第一个版本的self-attention'''\n",
    "\n",
    "####################\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    \"\"\"self-attention的第一重境界\"\"\"\n",
    "    def __init__(self, hidden_dim: int =728):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # * Q, K, V的权重矩阵\n",
    "        # * 假设x的shape是(128, 20), nn.Linear的dim是(20, 30), 最终计算结果是(128, 30)\n",
    "        self.query_Q_weight = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_K_weight = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_V_weight = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # * X shape is : (batch_size, seq_len, hidden_dim)\n",
    "        Q_matrix = self.query_Q_weight(x)\n",
    "        K_matrix = self.key_K_weight(x)\n",
    "        V_matrix = self.value_V_weight(x)\n",
    "        \n",
    "        # * Q, K, V shape is : (batch, seq_len, hidden_dim)\n",
    "        # * Attention weights shape is : (batch, seq_len, seq_len)\n",
    "        \n",
    "        attention_matrix = torch.matmul(Q_matrix, K_matrix.transpose(-1, -2))\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_matrix / math.sqrt(self.hidden_dim), dim=-1)\n",
    "        print(attention_weights)\n",
    "        # * output shape is : (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V_matrix)\n",
    "        \n",
    "                \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "X = torch.rand(3, 2, 4)\n",
    "\n",
    "# print(\"X: {}\".format(X))\n",
    "\n",
    "attention_V1 = SelfAttentionV1(4)\n",
    "\n",
    "attention_V1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5215, 0.4785],\n",
      "         [0.5253, 0.4747]],\n",
      "\n",
      "        [[0.5161, 0.4839],\n",
      "         [0.5132, 0.4868]],\n",
      "\n",
      "        [[0.5149, 0.4851],\n",
      "         [0.5078, 0.4922]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4113, -0.3819, -0.2678, -0.0727],\n",
       "         [ 0.4110, -0.3830, -0.2681, -0.0723]],\n",
       "\n",
       "        [[ 0.2601, -0.1874, -0.0258, -0.2135],\n",
       "         [ 0.2594, -0.1877, -0.0255, -0.2140]],\n",
       "\n",
       "        [[ 0.7261, -0.3526, -0.4820, -0.0516],\n",
       "         [ 0.7245, -0.3512, -0.4798, -0.0526]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"对原本的self-attention计算进行效率优化\"\"\"\n",
    "\n",
    "\n",
    "class SelfAttentionV2(nn.Module):\n",
    "    \"\"\"Some Information about SelfAttentionV2\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # * 将Q, K, V3个矩阵合并为一个大矩阵进行计算\n",
    "        self.cal_proj_weight = nn.Linear(hidden_dim, hidden_dim * 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # * X shape is : (batch, seq, dim)\n",
    "        # ! 这种方式只适用于模型比较小的时候\n",
    "        QKV_matrix = self.cal_proj_weight(x)\n",
    "        Q_matrix, K_matrix, V_matrix = torch.split(QKV_matrix, self.hidden_dim, dim=-1)\n",
    "        \n",
    "        \n",
    "        # * Q, K, V shape is : (batch, seq_len, hidden_dim)\n",
    "        # * Attention weights shape is : (batch, seq_len, seq_len)\n",
    "        \n",
    "        attention_matrix = torch.matmul(Q_matrix, K_matrix.transpose(-1, -2))\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_matrix / math.sqrt(self.hidden_dim), dim=-1)\n",
    "        print(attention_weights)\n",
    "        # * output shape is : (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V_matrix)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "X = torch.rand(3, 2, 4)\n",
    "\n",
    "attention_V2 = SelfAttentionV2(4)\n",
    "\n",
    "attention_V2(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''加入一些关于self-attention的细节'''\n",
    "\n",
    "# * 1. Dropout的位置\n",
    "# * 2. Attention mask, 因为在实际应用过程中，sequence的长度可能是不一样的\n",
    "# * 3. output 矩阵映射（可选）\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
