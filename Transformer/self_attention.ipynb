{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4960, 0.5040],\n",
      "         [0.4848, 0.5152]],\n",
      "\n",
      "        [[0.4944, 0.5056],\n",
      "         [0.4923, 0.5077]],\n",
      "\n",
      "        [[0.5221, 0.4779],\n",
      "         [0.5274, 0.4726]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1299, -0.6092, -0.4495, -0.2527],\n",
       "         [-0.1283, -0.6098, -0.4518, -0.2592]],\n",
       "\n",
       "        [[-0.2092, -0.8641, -0.7713, -0.2240],\n",
       "         [-0.2096, -0.8637, -0.7708, -0.2232]],\n",
       "\n",
       "        [[-0.1089, -0.6132, -0.4714, -0.2210],\n",
       "         [-0.1097, -0.6106, -0.4687, -0.2207]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''第一个版本的self-attention'''\n",
    "\n",
    "####################\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    \"\"\"self-attention的第一重境界\"\"\"\n",
    "    def __init__(self, hidden_dim: int =728):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # * Q, K, V的权重矩阵\n",
    "        # * 假设x的shape是(128, 20), nn.Linear的dim是(20, 30), 最终计算结果是(128, 30)\n",
    "        self.query_Q_weight = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_K_weight = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_V_weight = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # * X shape is : (batch_size, seq_len, hidden_dim)\n",
    "        Q_matrix = self.query_Q_weight(x)\n",
    "        K_matrix = self.key_K_weight(x)\n",
    "        V_matrix = self.value_V_weight(x)\n",
    "        \n",
    "        # * Q, K, V shape is : (batch, seq_len, hidden_dim)\n",
    "        # * Attention weights shape is : (batch, seq_len, seq_len)\n",
    "        \n",
    "        attention_matrix = torch.matmul(Q_matrix, K_matrix.transpose(-1, -2))\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_matrix / math.sqrt(self.hidden_dim), dim=-1)\n",
    "        print(attention_weights)\n",
    "        # * output shape is : (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V_matrix)\n",
    "        \n",
    "                \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "X = torch.rand(3, 2, 4)\n",
    "\n",
    "# print(\"X: {}\".format(X))\n",
    "\n",
    "attention_V1 = SelfAttentionV1(4)\n",
    "\n",
    "attention_V1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5215, 0.4785],\n",
      "         [0.5253, 0.4747]],\n",
      "\n",
      "        [[0.5161, 0.4839],\n",
      "         [0.5132, 0.4868]],\n",
      "\n",
      "        [[0.5149, 0.4851],\n",
      "         [0.5078, 0.4922]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4113, -0.3819, -0.2678, -0.0727],\n",
       "         [ 0.4110, -0.3830, -0.2681, -0.0723]],\n",
       "\n",
       "        [[ 0.2601, -0.1874, -0.0258, -0.2135],\n",
       "         [ 0.2594, -0.1877, -0.0255, -0.2140]],\n",
       "\n",
       "        [[ 0.7261, -0.3526, -0.4820, -0.0516],\n",
       "         [ 0.7245, -0.3512, -0.4798, -0.0526]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"对原本的self-attention计算进行效率优化\"\"\"\n",
    "\n",
    "\n",
    "class SelfAttentionV2(nn.Module):\n",
    "    \"\"\"Some Information about SelfAttentionV2\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # * 将Q, K, V3个矩阵合并为一个大矩阵进行计算\n",
    "        self.cal_proj_weight = nn.Linear(hidden_dim, hidden_dim * 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # * X shape is : (batch, seq, dim)\n",
    "        # ! 这种方式只适用于模型比较小的时候\n",
    "        QKV_matrix = self.cal_proj_weight(x)\n",
    "        Q_matrix, K_matrix, V_matrix = torch.split(QKV_matrix, self.hidden_dim, dim=-1)\n",
    "        \n",
    "        \n",
    "        # * Q, K, V shape is : (batch, seq_len, hidden_dim)\n",
    "        # * Attention weights shape is : (batch, seq_len, seq_len)\n",
    "        \n",
    "        attention_matrix = torch.matmul(Q_matrix, K_matrix.transpose(-1, -2))\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_matrix / math.sqrt(self.hidden_dim), dim=-1)\n",
    "        print(attention_weights)\n",
    "        # * output shape is : (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V_matrix)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "X = torch.rand(3, 2, 4)\n",
    "\n",
    "attention_V2 = SelfAttentionV2(4)\n",
    "\n",
    "attention_V2(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "tensor([[[0.3364, 0.3375, 0.3261, 0.0000],\n",
      "         [0.3198, 0.3440, 0.3362, 0.0000],\n",
      "         [0.3247, 0.3414, 0.3339, 0.0000],\n",
      "         [0.3294, 0.3390, 0.3315, 0.0000]],\n",
      "\n",
      "        [[0.5025, 0.4975, 0.0000, 0.0000],\n",
      "         [0.4829, 0.5171, 0.0000, 0.0000],\n",
      "         [0.4916, 0.5084, 0.0000, 0.0000],\n",
      "         [0.4966, 0.5034, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3273, -0.6831],\n",
       "         [-0.4053, -0.5083],\n",
       "         [-0.3276, -0.6836],\n",
       "         [-0.3275, -0.6833]],\n",
       "\n",
       "        [[-0.2727, -0.7993],\n",
       "         [-0.2698, -0.8035],\n",
       "         [-0.2711, -0.8017],\n",
       "         [-0.2718, -0.8006]],\n",
       "\n",
       "        [[-0.3080, -0.6561],\n",
       "         [-0.3080, -0.6561],\n",
       "         [-0.3080, -0.6561],\n",
       "         [-0.3080, -0.6561]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''加入一些关于self-attention的细节'''\n",
    "\n",
    "# * 1. Dropout的位置\n",
    "# * 2. Attention mask, 因为在实际应用过程中，sequence的长度可能是不一样的\n",
    "# * 3. output 矩阵映射（可选）\n",
    "\n",
    "class SelfAttentionV3(nn.Module):\n",
    "    \"\"\"Some Information about SelfAttentionV3\"\"\"\n",
    "    def __init__(self, hiddendim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.hiddendim = hiddendim\n",
    "        self.cal_matrix_weight = nn.Linear(hiddendim, hiddendim * 3)\n",
    "        self.dropout_layer = nn.Dropout(dropout_rate) # * 定义dropout\n",
    "        \n",
    "        # * 可选\n",
    "        self.output_mapping = nn.Linear(hiddendim, hiddendim)\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        QKV_matrix = self.cal_matrix_weight(x)\n",
    "        Q_matrix, K_matrix, V_matrix = torch.split(QKV_matrix, self.hiddendim, dim=-1)\n",
    "        \n",
    "        # * attention_matrix和attention_mask shape is (batch, seq, seq)\n",
    "        attention_matrix = Q_matrix @ K_matrix.transpose(-1, -2) / math.sqrt(self.hiddendim)\n",
    "        \n",
    "        # * mask要加在计算softmax之前\n",
    "        if attention_mask is not None:\n",
    "            attention_matrix = attention_matrix.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "             \n",
    "        # * Attention的Dropout是在Attention层面进行Dropout，因此将Dropout层加在计算完attention_weights之后\n",
    "        attention_weight = torch.softmax(attention_matrix, dim=-1)\n",
    "        print(attention_weight)\n",
    "        attention_weight = self.dropout_layer(attention_weight)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output = attention_weight @ V_matrix\n",
    "        \n",
    "        \n",
    "        output = self.output_mapping(output)\n",
    "            \n",
    "    \n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "X = torch.rand(3, 4, 2)\n",
    "mask = torch.tensor([[1,1,1,0],[1,1,0,0], [1,0,0,0]])\n",
    "\n",
    "print(mask.shape)\n",
    "\n",
    "mask = mask.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "print(mask.shape)\n",
    "attention_V3 = SelfAttentionV3(2)\n",
    "\n",
    "attention_V3(X, mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3425, 0.3439, 0.3136, 0.0000],\n",
      "         [0.3406, 0.3431, 0.3163, 0.0000],\n",
      "         [0.3471, 0.3366, 0.3164, 0.0000],\n",
      "         [0.3427, 0.3436, 0.3137, 0.0000]],\n",
      "\n",
      "        [[0.4733, 0.5267, 0.0000, 0.0000],\n",
      "         [0.4705, 0.5295, 0.0000, 0.0000],\n",
      "         [0.4783, 0.5217, 0.0000, 0.0000],\n",
      "         [0.4637, 0.5363, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5100, -0.8924],\n",
       "         [ 0.5101, -0.8929],\n",
       "         [ 0.5123, -0.8970],\n",
       "         [ 0.5101, -0.8926]],\n",
       "\n",
       "        [[ 0.3029, -0.4966],\n",
       "         [ 0.3026, -0.4956],\n",
       "         [ 0.3036, -0.4985],\n",
       "         [ 0.3017, -0.4929]],\n",
       "\n",
       "        [[ 0.4814, -0.9871],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.4814, -0.9871],\n",
       "         [ 0.4814, -0.9871]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''实际面试时的写法'''\n",
    "\n",
    "class SelfAttentionV4(nn.Module):\n",
    "    \"\"\"Some Information about SelfAttentionV4\"\"\"\n",
    "    def __init__(self, hiddendim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hiddendim = hiddendim\n",
    "        \n",
    "        self.query = nn.Linear(hiddendim, hiddendim)\n",
    "        self.key = nn.Linear(hiddendim, hiddendim)\n",
    "        self.value = nn.Linear(hiddendim, hiddendim)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \n",
    "        Q_matrix = self.query(x)\n",
    "        K_matrix = self.key(x)\n",
    "        V_matrix = self.value(x)\n",
    "        \n",
    "        attention_matrix = Q_matrix @ K_matrix.transpose(-1, -2) / math.sqrt(self.hiddendim)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_matrix = attention_matrix.masked_fill(attention_mask == 0, float(\"-inf\"))\n",
    "        \n",
    "        # * (batch, seq, seq)\n",
    "        attention_weights = torch.softmax(attention_matrix, dim=-1)\n",
    "        print(attention_weights)\n",
    "        attention_weights = self.dropout_layer(attention_weights)\n",
    "        \n",
    "        # * (batch, seq, hiddendim)\n",
    "        output = attention_weights @ V_matrix\n",
    "        \n",
    "        \n",
    "        return output\n",
    "\n",
    "X = torch.rand(3, 4, 2)\n",
    "\n",
    "mask = torch.tensor([[1,1,1,0],[1,1,0,0],[1,0,0,0]])\n",
    "\n",
    "mask = mask.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "\n",
    "attention_V4 = SelfAttentionV4(2)\n",
    "\n",
    "attention_V4(X, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 2, 16])\n",
      "torch.Size([3, 8, 2, 2])\n",
      "tensor([[[[0., 1.],\n",
      "          [0., 1.]],\n",
      "\n",
      "         [[0., 1.],\n",
      "          [0., 1.]],\n",
      "\n",
      "         [[0., 1.],\n",
      "          [0., 1.]],\n",
      "\n",
      "         [[0., 1.],\n",
      "          [0., 1.]],\n",
      "\n",
      "         [[0., 1.],\n",
      "          [0., 1.]],\n",
      "\n",
      "         [[0., 1.],\n",
      "          [0., 1.]],\n",
      "\n",
      "         [[0., 1.],\n",
      "          [0., 1.]],\n",
      "\n",
      "         [[0., 1.],\n",
      "          [0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[1., 0.],\n",
      "          [1., 0.]],\n",
      "\n",
      "         [[1., 0.],\n",
      "          [1., 0.]],\n",
      "\n",
      "         [[1., 0.],\n",
      "          [1., 0.]],\n",
      "\n",
      "         [[1., 0.],\n",
      "          [1., 0.]],\n",
      "\n",
      "         [[1., 0.],\n",
      "          [1., 0.]],\n",
      "\n",
      "         [[1., 0.],\n",
      "          [1., 0.]],\n",
      "\n",
      "         [[1., 0.],\n",
      "          [1., 0.]],\n",
      "\n",
      "         [[1., 0.],\n",
      "          [1., 0.]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5538e-02,  1.2561e-02, -3.7610e-01, -3.6960e-02,  2.0556e-01,\n",
       "           2.5628e-01,  1.7213e-01, -9.5748e-02,  4.5065e-03,  1.9761e-01,\n",
       "          -3.1898e-01, -2.4191e-01, -1.7611e-01, -2.6347e-01,  2.5076e-02,\n",
       "           1.6802e-01, -8.6805e-02, -1.4227e-01, -1.8106e-02,  1.9347e-01,\n",
       "           1.2386e-01, -4.8278e-02,  1.5668e-01,  1.7373e-01,  1.6753e-02,\n",
       "          -7.4336e-02,  3.7434e-01, -1.3460e-01,  7.7500e-02,  1.0456e-01,\n",
       "          -1.5986e-01, -9.7740e-02,  3.9382e-01,  1.5115e-01, -5.8893e-02,\n",
       "          -1.0212e-01, -9.5123e-02, -2.5547e-01,  1.0127e-01, -8.6984e-02,\n",
       "           1.8273e-01,  1.8470e-01,  1.2324e-01,  3.0143e-01, -8.1672e-02,\n",
       "          -3.8502e-01,  4.3018e-01, -1.8792e-01,  1.6497e-01, -2.5447e-02,\n",
       "          -2.2439e-01, -3.1514e-01, -2.9029e-01,  2.7727e-02,  2.4627e-01,\n",
       "           3.2085e-02,  9.1385e-02,  2.7959e-01, -7.4282e-02, -1.3875e-01,\n",
       "          -1.8987e-01, -2.2204e-01, -3.4076e-01,  2.2504e-01, -4.8415e-02,\n",
       "           4.3101e-01,  1.3714e-01, -3.5215e-02,  2.8233e-01,  2.6569e-02,\n",
       "           1.7318e-01,  3.7383e-02,  1.2255e-01,  1.3175e-01, -3.0211e-01,\n",
       "          -3.9303e-03,  1.4281e-01,  4.9897e-02, -1.1782e-02, -2.8581e-01,\n",
       "           3.2076e-01,  3.2872e-01, -2.9468e-01, -2.3222e-01,  6.3544e-03,\n",
       "           2.0232e-01,  2.8727e-01, -2.1233e-01,  3.7988e-02, -7.3819e-02,\n",
       "           8.6212e-02, -1.1322e-01,  3.8892e-02,  2.0162e-01,  1.6967e-01,\n",
       "          -1.3371e-01, -3.1761e-02, -7.2232e-02, -3.1514e-01, -1.3326e-01,\n",
       "           4.8400e-02,  1.0681e-01,  9.3801e-02, -7.4153e-02, -3.0154e-01,\n",
       "          -5.2067e-02, -1.8082e-01,  1.2535e-01,  4.4601e-02,  2.6497e-01,\n",
       "          -4.7876e-02,  1.7917e-01,  2.1729e-01, -3.3870e-01,  1.2238e-01,\n",
       "          -2.8269e-01, -2.5485e-01, -7.9356e-02, -1.4010e-01,  3.5243e-01,\n",
       "          -3.8155e-01,  2.7988e-02,  1.3967e-02, -1.4568e-01,  1.2571e-01,\n",
       "           3.0112e-01,  4.8309e-03, -5.0521e-03],\n",
       "         [-1.9139e-02,  6.6707e-02, -3.1422e-01, -3.8845e-02,  1.0116e-01,\n",
       "           8.1949e-02,  2.5512e-01, -1.3586e-01, -8.4557e-02,  2.7533e-01,\n",
       "          -2.1707e-01, -1.1084e-01, -3.1374e-01, -4.9189e-01,  2.2693e-01,\n",
       "           3.2042e-01, -1.9707e-01, -1.7353e-01,  2.4406e-02,  2.2694e-01,\n",
       "          -6.6896e-02, -1.3770e-01,  1.6651e-01,  9.5371e-02,  4.1825e-01,\n",
       "           5.7390e-02,  1.8795e-01, -3.3039e-01,  1.1397e-01,  3.9306e-02,\n",
       "          -1.2945e-01, -1.3612e-01, -1.0323e-01,  2.6703e-01,  1.8716e-01,\n",
       "          -3.3931e-02, -3.3025e-02, -2.3685e-01,  8.8326e-02,  7.5373e-03,\n",
       "           9.3545e-02,  1.2683e-01,  4.2731e-02,  1.8251e-01,  2.1969e-03,\n",
       "          -2.9736e-01,  3.4359e-01,  7.2584e-02,  1.5612e-01,  1.4970e-02,\n",
       "          -1.9424e-01, -3.9049e-01, -2.0473e-01, -1.0671e-01,  2.6121e-01,\n",
       "          -1.4013e-01,  3.1613e-02,  1.2979e-01, -2.1607e-02,  4.6741e-02,\n",
       "          -2.5890e-01,  8.1880e-02, -4.2645e-01,  1.0246e-01, -4.6147e-03,\n",
       "           1.7866e-01,  2.5652e-01, -9.6545e-02,  1.8641e-01, -1.4066e-02,\n",
       "          -3.0642e-02,  6.1418e-02, -9.3395e-02, -2.4104e-02, -2.1367e-01,\n",
       "          -1.5563e-01,  9.2498e-02, -3.8579e-02, -3.2653e-01, -2.0974e-01,\n",
       "           1.1773e-01,  3.9051e-01, -3.3554e-01, -2.9291e-01, -5.0607e-02,\n",
       "          -1.5260e-01,  1.9580e-01,  5.2583e-02,  1.0057e-01, -4.1677e-02,\n",
       "           1.1986e-01, -1.9884e-01,  4.8382e-02,  1.0953e-01,  2.4484e-02,\n",
       "           2.1072e-01, -1.0787e-01, -5.8488e-02, -2.5957e-01, -1.2918e-01,\n",
       "           2.1623e-01,  2.5067e-01,  2.0246e-01, -2.4278e-01, -3.3319e-03,\n",
       "          -1.6986e-01, -2.9641e-01,  1.7002e-01,  8.5508e-02,  2.6387e-01,\n",
       "          -6.9053e-02,  5.3759e-02,  1.3078e-01,  1.9206e-02, -4.2616e-02,\n",
       "          -3.7520e-01, -2.7398e-01, -1.7860e-02,  6.3736e-02,  3.9952e-01,\n",
       "          -3.7524e-01, -1.8213e-01,  6.6811e-02,  3.3892e-03,  2.1093e-01,\n",
       "           1.4863e-01,  8.0921e-02,  1.6128e-01]],\n",
       "\n",
       "        [[        nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan],\n",
       "         [        nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan]],\n",
       "\n",
       "        [[-1.0980e-01,  2.3477e-01, -2.1157e-01,  2.5985e-01,  1.0736e-03,\n",
       "          -1.4550e-01, -3.4124e-02, -2.3685e-01,  8.7696e-03,  3.2988e-01,\n",
       "          -2.7847e-01,  3.5239e-02, -1.1929e-01, -4.4609e-01,  1.8176e-01,\n",
       "           3.1848e-01, -2.1035e-01,  2.0510e-01,  3.7549e-01,  2.8346e-01,\n",
       "           1.8805e-02, -2.8218e-01,  3.9030e-02,  3.5819e-01,  3.6920e-01,\n",
       "           3.1281e-02,  1.5189e-01, -1.0732e-01, -7.7883e-02,  2.2495e-01,\n",
       "          -2.4494e-01, -1.3158e-01, -2.6108e-01, -3.1251e-04,  1.9954e-01,\n",
       "           1.7646e-02, -2.6442e-01, -2.8185e-01,  3.2322e-02, -3.1333e-02,\n",
       "          -9.3398e-03,  3.0761e-01,  2.5905e-01, -1.8891e-01,  1.6804e-02,\n",
       "          -1.8442e-01,  2.2244e-01,  8.6266e-02,  1.2383e-01, -2.0594e-01,\n",
       "          -4.4534e-02, -3.1876e-01, -1.2113e-01,  1.6253e-01,  1.6327e-02,\n",
       "          -6.4624e-02,  1.9535e-01,  1.5179e-01, -1.0776e-01,  2.5539e-01,\n",
       "          -3.8617e-02,  3.8054e-01, -4.2075e-01,  1.5807e-01, -9.2111e-02,\n",
       "           1.0672e-01,  1.6905e-02, -3.1642e-01, -1.5155e-02,  1.9640e-02,\n",
       "           2.2687e-01,  1.7003e-01,  2.9792e-02, -1.3212e-02,  8.1853e-03,\n",
       "           1.4231e-02,  1.8911e-02,  1.5369e-01,  3.1518e-02, -1.1219e-01,\n",
       "           2.3440e-01,  2.4489e-01, -2.3200e-01, -2.2079e-01, -1.6333e-01,\n",
       "          -3.9491e-04,  2.9374e-01, -3.0444e-01, -1.1837e-01, -1.2151e-01,\n",
       "          -1.3308e-01, -1.3584e-01,  1.2422e-01,  1.1703e-01, -5.7863e-02,\n",
       "           4.8157e-03, -2.0233e-01,  6.1776e-02, -1.7531e-01,  5.1530e-02,\n",
       "          -2.4153e-02,  4.1532e-01,  1.2191e-01, -6.9544e-02,  9.5901e-02,\n",
       "          -1.7038e-01, -2.4190e-01, -1.2770e-01,  1.6993e-01, -1.2458e-02,\n",
       "          -1.7059e-01,  3.1496e-01,  9.5865e-02, -1.4723e-01,  2.9450e-01,\n",
       "          -4.0675e-01, -1.9097e-01,  5.0708e-02,  1.7744e-02,  2.0854e-01,\n",
       "          -3.1210e-01, -3.6505e-02,  2.2118e-01,  2.2482e-01, -1.1224e-02,\n",
       "           1.1734e-01,  5.7085e-02,  1.7613e-02],\n",
       "         [-7.8945e-02,  1.9787e-01, -2.7836e-01,  3.1332e-01, -3.8879e-02,\n",
       "          -2.2854e-01, -4.3385e-02, -2.5067e-01,  4.9048e-02,  1.3403e-01,\n",
       "          -3.2215e-01,  7.0099e-02, -1.6856e-01, -4.5950e-01,  2.3024e-01,\n",
       "           1.0938e-01, -2.0841e-01,  2.3520e-01,  5.2613e-01,  3.7244e-01,\n",
       "          -7.8561e-02, -3.1662e-01,  1.3818e-01,  1.0941e-01,  2.5820e-01,\n",
       "           3.2045e-02,  1.4648e-01, -2.9062e-02,  2.0677e-01,  2.7317e-02,\n",
       "          -1.5843e-01, -3.1011e-02, -2.2456e-01, -5.4046e-02,  2.0164e-01,\n",
       "           1.0060e-01, -3.4524e-01, -2.9338e-01,  3.6695e-02, -3.6327e-01,\n",
       "          -8.0751e-02,  2.2394e-01,  9.9575e-02, -1.1393e-01,  1.0881e-01,\n",
       "          -8.5719e-02,  2.2024e-01,  4.2631e-02,  3.6147e-01, -2.3162e-01,\n",
       "           1.7424e-01, -4.5271e-01, -1.3108e-01,  1.6775e-01, -2.3775e-02,\n",
       "          -1.5172e-01,  1.9454e-01,  1.8419e-01, -1.7392e-01,  1.4819e-01,\n",
       "          -6.1922e-02,  2.6416e-01, -2.9423e-01,  1.6480e-01, -1.0487e-01,\n",
       "           3.6526e-01,  3.3458e-02, -7.7983e-02,  1.3594e-01,  2.6412e-01,\n",
       "           3.3393e-01,  1.2061e-01,  1.1626e-01, -9.1150e-02, -2.2295e-01,\n",
       "          -1.0674e-01,  7.0886e-02,  2.1670e-01,  5.2210e-02, -1.5172e-01,\n",
       "           1.1095e-01,  2.3228e-01, -3.2058e-01, -3.8918e-02, -4.3693e-03,\n",
       "           3.7021e-02,  1.8751e-01, -4.2639e-01, -8.6887e-02,  2.8649e-03,\n",
       "          -1.5565e-01, -1.1702e-01,  1.1698e-01,  1.4084e-01,  3.0453e-02,\n",
       "           1.1384e-01, -3.5361e-01,  1.7614e-01, -3.0659e-02,  3.0913e-01,\n",
       "          -1.4921e-01,  4.7699e-01,  9.1299e-02, -2.4487e-01, -9.2968e-02,\n",
       "          -3.6514e-01, -2.6929e-01, -3.3266e-02,  1.8358e-01, -1.7699e-01,\n",
       "          -1.7236e-01,  3.0482e-01, -1.1699e-01, -1.6176e-01,  2.3866e-01,\n",
       "          -2.6419e-01,  9.7642e-02,  1.0412e-01, -7.4109e-02,  1.1353e-01,\n",
       "          -3.6090e-01, -1.6455e-01,  5.6341e-02,  1.5430e-01,  1.9656e-01,\n",
       "           1.5985e-01,  1.0725e-01,  2.9209e-02]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''多头注意力机制(Multi-head Attention)'''\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Some Information about MultiHeadAttention\"\"\"\n",
    "    def __init__(self, hiddendim, head_num, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.hiddendim = hiddendim\n",
    "        self.head_num = head_num\n",
    "        self.headdim = hiddendim // head_num  # * (head_num * headdim = hiddendim)\n",
    "        \n",
    "        \n",
    "        self.query = nn.Linear(hiddendim, hiddendim) # * 实际上是(hiddendim, headdim * head_num)\n",
    "        self.key = nn.Linear(hiddendim, hiddendim) # * 实际上是(hiddendim, headdim * head_num)\n",
    "        self.value = nn.Linear(hiddendim, hiddendim) # * 实际上是(hiddendim, headdim * head_num)\n",
    "        self.output = nn.Linear(hiddendim, hiddendim) \n",
    "        \n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \n",
    "        # * 记录x的shape (batch seq, hiddendim)\n",
    "        batch, seq_len, _ = x.size()\n",
    "        \n",
    "        Q_matrix = self.query(x)\n",
    "        K_matrix = self.key(x)\n",
    "        V_matrix = self.value(x)\n",
    "        \n",
    "        # * (batch seq, hiddendim) => (batch, head_num, seq, head_dim)\n",
    "        # * 其实是将hiddendim维度拆成 head_num * head_dim\n",
    "        \n",
    "        Q_head_state = Q_matrix.view(batch, seq_len, self.head_num, self.headdim).transpose(2, 1)\n",
    "        K_head_state = K_matrix.view(batch, seq_len, self.head_num, self.headdim).transpose(2, 1)\n",
    "        V_head_state = V_matrix.view(batch, seq_len, self.head_num, self.headdim).transpose(2, 1)\n",
    "        print(Q_head_state.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # * attention_matrix shape is (batch, head_sum, seq, seq)\n",
    "        attention_matrix = Q_head_state @ K_head_state.transpose(-1, -2) / math.sqrt(self.headdim)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_matrix = attention_matrix.masked_fill(attention_mask == 0, float(\"-inf\"))\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_matrix, dim=-1)\n",
    "        print(attention_weights.shape)\n",
    "        print(attention_weights)\n",
    "        \n",
    "        attention_weights = self.attention_dropout(attention_weights)\n",
    "        \n",
    "        # * output_mid shape is (batch, head_sum, seq, head_dim)\n",
    "        output_mid = attention_weights @ V_head_state\n",
    "        \n",
    "        # * reshape from (batch, head_sum, seq, head_dim) to (batch, seq, hiddendim)\n",
    "        output_mid = output_mid.transpose(2, 1).contiguous()\n",
    "        output_mid = output_mid.view(batch, seq_len, -1)\n",
    "        \n",
    "        output = self.output(output_mid)\n",
    "        return output\n",
    "\n",
    "# * (3, 2) => (3, 1, 2) => (3, 1, 1, 2) => (3, 8, 2, 2)\n",
    "attention_mask = torch.tensor([[0,1], [0,0], [1,0]]).unsqueeze(1).unsqueeze(2).expand(3,8,2,2)\n",
    "\n",
    "X = torch.rand(3,2,128) # * headdim = 16\n",
    "\n",
    "\n",
    "\n",
    "head_attention = MultiHeadAttention(128,8)\n",
    "\n",
    "\n",
    "head_attention(X, attention_mask)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
