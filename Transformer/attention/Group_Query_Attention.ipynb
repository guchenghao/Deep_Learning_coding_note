{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupQueryAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 128])\n"
     ]
    }
   ],
   "source": [
    "# * 忽略dropour layer和attention_mask\n",
    "# * GroupQueryAttention是为了解决K/V Cache存储的问题，让多个Query共享同一组Key/Value值\n",
    "class GroupQueryAttention(nn.Module):\n",
    "    \"\"\"Some Information about GroupQueryAttention\"\"\"\n",
    "    def __init__(self, hidden_dim, nums_head, nums_key_value_head):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % nums_head == 0\n",
    "        assert nums_head % nums_key_value_head == 0\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.nums_head = nums_head\n",
    "        self.head_dim = hidden_dim // nums_head\n",
    "        self.nums_key_value_head = nums_key_value_head  # * 这个参数表示将nums_head分为多少组，也表示总共我们有多少组key/value矩阵\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, nums_key_value_head * self.head_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, nums_key_value_head * self.head_dim)\n",
    "        self.output_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # * x shape is: (batch, seq, hidden_dim)\n",
    "\n",
    "        batch, seq, _ = x.size()\n",
    "\n",
    "        q_matrix = self.q_proj(x)\n",
    "        k_matrix = self.k_proj(x)\n",
    "        v_matrix = self.v_proj(x)\n",
    "\n",
    "        # * q_head_matrix: (batch, nums_head, seq, head_dim)\n",
    "        q_head_matrix = q_matrix.view(batch, seq, self.nums_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # * k_head_matrix and v_head_matrix: (batch, nums_key_value_head, seq, head_dim)\n",
    "        k_head_matrix = k_matrix.view(batch, seq, self.nums_key_value_head, self.head_dim).transpose(1, 2)\n",
    "        v_head_matrix = v_matrix.view(batch, seq, self.nums_key_value_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # * 对key和value矩阵进行repeat，确保2个张量的head数量是一致的\n",
    "        # * 为什么这里是用repeat_interleave是因为repeat_interleave是针对张量中的元素进行操作，而repeat函数是对整个张量在不同维度上进行重复\n",
    "        # * 我们这里只需要对每组key/value矩阵，重复self.nums_head // self.nums_key_value_head次数，满足每个head的query能够对应进行相乘的key/value矩阵\n",
    "        # * k_head_matrix and v_head_matrix: (batch, nums_head, seq, head_dim)\n",
    "        k_head_matrix = torch.repeat_interleave(k_head_matrix, repeats=self.nums_head // self.nums_key_value_head, dim=1)\n",
    "        v_head_matrix = torch.repeat_interleave(v_head_matrix, repeats=self.nums_head // self.nums_key_value_head, dim=1)\n",
    "\n",
    "        # * attention_matrix: (batch, nums_head, seq, seq)\n",
    "        attention_matrix = q_head_matrix @ k_head_matrix.transpose(2, 3) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # * 这里忽略attention mask\n",
    "\n",
    "        # * attention_weight: (batch, nums_head, seq, seq)\n",
    "        attention_weight = torch.softmax(attention_matrix, dim=-1)\n",
    "\n",
    "        # * 这里也忽略了dropout层\n",
    "\n",
    "        # * mid_output: (batch, nums_head, seq, head_dim)\n",
    "        mid_output = attention_weight @ v_head_matrix\n",
    "\n",
    "        mid_output = mid_output.transpose(1, 2).contiguous()\n",
    "\n",
    "        mid_output = mid_output.view(batch, seq, -1)\n",
    "\n",
    "        output = self.output_proj(mid_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = torch.rand(3, 2, 128)\n",
    "net = GroupQueryAttention(128, 8, 4)\n",
    "\n",
    "print(net(x).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
