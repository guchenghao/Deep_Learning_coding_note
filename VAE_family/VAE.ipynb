{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2.9726\n",
      "Epoch [2/20], Loss: 2.4362\n",
      "Epoch [3/20], Loss: 1.9719\n",
      "Epoch [4/20], Loss: 1.5649\n",
      "Epoch [5/20], Loss: 1.2141\n",
      "Epoch [6/20], Loss: 0.9195\n",
      "Epoch [7/20], Loss: 0.6759\n",
      "Epoch [8/20], Loss: 0.4841\n",
      "Epoch [9/20], Loss: 0.3392\n",
      "Epoch [10/20], Loss: 0.2341\n",
      "Epoch [11/20], Loss: 0.1610\n",
      "Epoch [12/20], Loss: 0.1119\n",
      "Epoch [13/20], Loss: 0.0768\n",
      "Epoch [14/20], Loss: 0.0529\n",
      "Epoch [15/20], Loss: 0.0362\n",
      "Epoch [16/20], Loss: 0.0247\n",
      "Epoch [17/20], Loss: 0.0164\n",
      "Epoch [18/20], Loss: 0.0112\n",
      "Epoch [19/20], Loss: 0.0078\n",
      "Epoch [20/20], Loss: 0.0054\n"
     ]
    }
   ],
   "source": [
    "class Simple_VAE(nn.Module):\n",
    "    \"\"\"Some Information about Simple_VAE\"\"\"\n",
    "    def __init__(self, input_dim, latentdim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        hiddendim = 64\n",
    "        # * 输入是默认将图片展成1维的向量\n",
    "        # * 这里面是用简单的DNN来构建encoder和decoder, 实际上可以采用更复杂的网络结构，resnet或者其他blocks\n",
    "        self.encoder_l1 = nn.Linear(input_dim, hiddendim)\n",
    "        self.encoder_lodvar = nn.Linear(hiddendim, latentdim)\n",
    "        self.encoder_mu = nn.Linear(hiddendim, latentdim)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.decoder_l1 = nn.Linear(latentdim, hiddendim)\n",
    "        self.decoder_l2 = nn.Linear(hiddendim, input_dim)\n",
    "        self.decoder_activation = nn.Sigmoid()\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        hidden_output = self.encoder_l1(x)\n",
    "        hidden_output = self.activation(hidden_output)\n",
    "        \n",
    "        return self.encoder_mu(hidden_output), self.encoder_lodvar(hidden_output)\n",
    "    \n",
    "    # * 重参数化技巧\n",
    "    # * logvar的dimention和mu的dimention是一样的\n",
    "    # * std_var = exp(log(var) / 2)\n",
    "    # * 直接输出 log(variance) 作为模型参数，可以避免对非负方差的显式约束，简化优化过程, 可以将优化问题转化为无界优化 (variance >= 0)\n",
    "    def sample_z(self, mu, logvar):\n",
    "        # * 利用log(variance)计算标准差\n",
    "        std_var = torch.exp(0.5 * logvar)\n",
    "        eps = torch.rand_like(std_var) # * 从标准正态分布中随机出一个张量，给生成过程赋予随机性\n",
    "        \n",
    "        return mu + eps * std_var\n",
    "    \n",
    "    \n",
    "    def decoder(self, z):\n",
    "        \n",
    "        decoder_hidden = self.decoder_l1(z)\n",
    "        decoder_hidden = self.activation(decoder_hidden)\n",
    "        decoder_output = self.decoder_l2(decoder_hidden)\n",
    "        decoder_output = self.decoder_activation(decoder_output)\n",
    "        \n",
    "        return decoder_output\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        mu, logvar = self.encoder(x.view(-1, self.input_dim))\n",
    "        \n",
    "        z = self.sample_z(mu, logvar)\n",
    "        \n",
    "        output = self.decoder(z)\n",
    "        return output, mu, logvar\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar, input_dim):\n",
    "    mse_loss = nn.MSELoss()\n",
    "    reconstruction_loss = mse_loss(recon_x, x.view(-1, input_dim))\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return reconstruction_loss + KLD\n",
    "\n",
    "\n",
    "# * 随机创建3个batch的数据，batch_size为5\n",
    "train_X = torch.randint(0, 1, (3, 5, 10, 10)).float()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Simple_VAE(100, 128)\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train() # * 设置训练模式\n",
    "train_loss = 0\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, data in enumerate(train_X):\n",
    "        \n",
    "        # * 前向传播，并且计算loss\n",
    "        x = data\n",
    "        recon_x, mu, logvar = model(x)\n",
    "        loss = loss_function(recon_x, x, mu, logvar, 100)\n",
    "        \n",
    "        optimizer_adam.zero_grad() # * 清空梯度\n",
    "        loss.backward() # * 反向传播 loss, 计算梯度\n",
    "        train_loss += loss.item()\n",
    "        optimizer_adam.step() # * 更新参数\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
